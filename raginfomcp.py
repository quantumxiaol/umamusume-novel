"""
RAG MCP
python raginfomcp.py -p 7778
to activate the MCP server

"""
import io
import os
import sys
import uvicorn
import json
import argparse
import uvicorn

from langchain.prompts import PromptTemplate
from langchain.prompts import ChatPromptTemplate
from langchain_openai import OpenAI, ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.schema.runnable import RunnableSequence
from typing import TypedDict, List
from langgraph.prebuilt import create_react_agent
from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from dashscope import embeddings
from langchain_community.document_loaders import TextLoader,CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from starlette.middleware.cors import CORSMiddleware
from langchain_community.document_loaders import PyMuPDFLoader
from langchain import hub
from langchain.schema import Document
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

load_dotenv(".env")

model=os.getenv("QWEN_MODEL_NAME")
api_key=os.getenv("QWEN_MODEL_API_KEY")
api_base=os.getenv("QWEN_MODEL_BASE_URL")

ua=os.getenv("USER_AGENT")

# åˆå§‹åŒ– LLM æ¨¡å‹
model = ChatOpenAI(
    model_name= model,
    api_key= api_key,
    base_url=api_base,
)
description = """
è¿™æ˜¯ä¸€ä¸ªå…³äºèµ›é©¬å¨˜è§’è‰²ä¿¡æ¯çš„è¡¨æ ¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- èµ›é©¬å¨˜ä¸­æ–‡å: è§’è‰²çš„ä¸­æ–‡åç§°
- èµ›é©¬å¨˜è‹±è¯­å / æ—¥è¯­å: å¯¹åº”è‹±æ–‡å’Œæ—¥æ–‡å
- ç”Ÿæ—¥: å‡ºç”Ÿæ—¥æœŸ
- ä¸‰å›´: BWH å°ºå¯¸
- èº«é«˜(cm): å•ä½å˜ç±³
- å£°ä¼˜(cv): é…éŸ³æ¼”å‘˜
- èµ›é©¬å¨˜ç‰¹æ®Šç§°å·(ä¸­æ–‡å/æ—¥è¯­å): ç‰¹æ®Šç§°å·åŠæ—¥æ–‡åï¼Œä¹Ÿå«ä¸“å±ç§°å·ï¼Œæˆ–äºŒã¤åã€åˆ«ç§°
- èµ›é©¬å¨˜ç‰¹æ®Šç§°å·è·å–æ¡ä»¶: è·å–è¯¥ç§°å·çš„å…·ä½“æ¸¸æˆå†…æ¡ä»¶

ä¾‹å¦‚ï¼šç±³æµ´çš„ç§°å·â€œæ¼†é»‘åˆºå®¢â€çš„è·å–æ¡ä»¶æ˜¯ï¼š
é‡èµæ¯”èµ›å‡ºæˆ˜23æ¬¡ä»¥ä¸Šï¼Œåœ¨äººæ°”ç¬¬äºŒä»¥ä¸Šçš„æƒ…å†µä¸‹å–å¾—èŠèŠ±è³ã€å¤©çš‡è³(æ˜¥)çš„èƒœåˆ©ï¼›ç²‰ä¸æ•°è¾¾åˆ°320,000ä»¥ä¸Šã€‚
"""
metadata_doc = Document(page_content=description, metadata={"source": "table_description"})

# åŠ è½½æ–‡æ¡£,å¯æ¢æˆPDFã€txtã€docç­‰å…¶ä»–æ ¼å¼æ–‡æ¡£
loader = CSVLoader('./docs/umamusume_character_baseinfo.csv', encoding='utf-8')
csv_documents = loader.load()
# for doc in csv_documents[:5]: 
#     print("-----")
#     print(doc.page_content)
md_loader = TextLoader('./docs/umamusume_game_info.md', encoding='utf-8')
md_documents = md_loader.load()

documents = [metadata_doc] + csv_documents + md_documents
# text_splitter = RecursiveCharacterTextSplitter.from_language(language="markdown", chunk_size=200, chunk_overlap=0)
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0,separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""])
separators=["\n", "ã€‚", "ï¼Œ", " ", ""]
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    length_function=len,
    add_start_index=True,
    separators=separators,
)


pages = text_splitter.split_documents(documents)

# åŠ è½½PDF
# loader = PyMuPDFLoader("")
# pages = loader.load_and_split()


texts = text_splitter.create_documents(
    [page.page_content for page in pages]
)

# print(f"å…±åˆ‡åˆ†äº† {len(texts)} ä¸ªæ–‡æ¡£å—")
# for i, text in enumerate(texts[:3]):
#     print(f"ç¬¬ {i} ä¸ªæ–‡æœ¬å†…å®¹: {text.page_content[:100]}...")  # æ‰“å°å‰100å­—ç¬¦

# é€‰æ‹©å‘é‡æ¨¡å‹ï¼Œå¹¶çŒåº“
# oa_embeddings = QwenEmbeddings(
#     model="text-embedding-ada-002",
#     api_key=open_api_key,
#     base_url=openai_api_base,
# )

# qw_embedding_model = QwenEmbeddings(
#     model_name="text-embedding-v2", 
#     api_key=os.getenv("QWEN_API_KEY"),
#     )

hf_embedding_model = HuggingFaceEmbeddings(
    model_name=os.getenv("HF_Embedding_Model"),
    model_kwargs = {'device': 'cpu'},
    encode_kwargs = {'normalize_embeddings': True}
    )
db = FAISS.from_documents(texts, hf_embedding_model)


# è·å–æ£€ç´¢å™¨ï¼Œé€‰æ‹© top-2 ç›¸å…³çš„æ£€ç´¢ç»“æœ
retriever = db.as_retriever(search_kwargs={"k": 10})

# åˆ›å»ºå¸¦æœ‰ system æ¶ˆæ¯çš„æ¨¡æ¿
prompt_template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªå¯¹æ¥é—®é¢˜æ’æŸ¥æœºå™¨äººã€‚
               ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
               ç¡®ä¿ä½ çš„å›å¤å®Œå…¨ä¾æ®ä¸‹è¿°å·²çŸ¥ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
               è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

               å·²çŸ¥ä¿¡æ¯:
               {context} """),
    ("user", "{question}")
])

# è‡ªå®šä¹‰çš„æç¤ºè¯å‚æ•°
chain_type_kwargs = {
    "prompt": prompt_template,
}

# å®šä¹‰RetrievalQAé“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=model,
    chain_type="stuff",  # ä½¿ç”¨stuffæ¨¡å¼å°†ä¸Šä¸‹æ–‡æ‹¼æ¥åˆ°æç¤ºè¯ä¸­
    chain_type_kwargs=chain_type_kwargs,
    retriever=retriever
)

# æ„å»º FastAPI åº”ç”¨ï¼Œæä¾›æœåŠ¡
app = FastAPI(title="LangChain-Umamusume-Server")

# å¯é€‰ï¼Œå‰ç«¯æŠ¥CORSæ—¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)


# å®šä¹‰è¯·æ±‚æ¨¡å‹
class QuestionRequest(BaseModel):
    question: str

# å®šä¹‰å“åº”æ¨¡å‹
class AnswerResponse(BaseModel):
    answer: str


# æä¾›æŸ¥è¯¢æ¥å£ http://127.0.0.1:1145/ask
@app.post("/raginfo", response_model=AnswerResponse,
          description="""
            Use Local Vector Database to answer questions about Umamusume characters.
            contain accurate and basic information about Umamusume characters.
            input: AnswerResponse
            Parameters:{question: str}
            output: AnswerResponse
            Example:{
                "question": "What is the birthday of Rice Shower?"
            }
            Result:{
                "answer": "Rice Shower was born on 3/05."
            }
            """
          
          )
async def rag(request: QuestionRequest):
    try:
        # è·å–ç”¨æˆ·é—®é¢˜
        user_question = request.question
        print(user_question)

        # é€šè¿‡RAGé“¾ç”Ÿæˆå›ç­”
        raw_answer = qa_chain.invoke(user_question)

        if isinstance(raw_answer, dict):
            answer_text = raw_answer.get("result", str(raw_answer))  # æå– result å­—æ®µ
        else:
            answer_text = raw_answer

        # è¿”å›ç­”æ¡ˆ
        answer = AnswerResponse(answer=answer_text)
        print(answer)
        return answer
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
mcp = FastApiMCP(app)
mcp.mount(mount_path="/mcp")

if __name__ == "__main__":
    arg=argparse.ArgumentParser()
    
    arg.add_argument("--port","-p", type=int, default=7778)
    args=arg.parse_args()
    print(f"ğŸš€ Starting server on port {args.port}")

    uvicorn.run("raginfomcp:app", 
                host="0.0.0.0", 
                port=args.port, 
                # reload=True
                )
    for route in app.routes:
        print(route.path)