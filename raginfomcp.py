"""
RAG MCP
python raginfomcp.py --http -p 7778
to activate the MCP server

"""
import io
import os
import sys
import uvicorn
import json
import argparse
import uvicorn

from langchain.prompts import PromptTemplate
from langchain.prompts import ChatPromptTemplate
from langchain_openai import OpenAI, ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.schema.runnable import RunnableSequence
from typing import TypedDict, List
from langgraph.prebuilt import create_react_agent
from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from dashscope import embeddings
from langchain_community.document_loaders import TextLoader,CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from starlette.middleware.cors import CORSMiddleware
from langchain_community.document_loaders import PyMuPDFLoader
from langchain import hub
from langchain.schema import Document
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP
import argparse
import uvicorn
import contextlib
from mcp.server import FastMCP
from collections.abc import AsyncIterator
from mcp.server.fastmcp import FastMCP
from starlette.applications import Starlette
from mcp.server.sse import SseServerTransport
from starlette.requests import Request
from starlette.routing import Mount, Route
from starlette.types import Receive, Scope, Send
from mcp.server import Server
from mcp.server.streamable_http_manager import StreamableHTTPSessionManager

from dotenv import load_dotenv
from fastapi import FastAPI


load_dotenv(".env")

model=os.getenv("QWEN_MODEL_NAME")
api_key=os.getenv("QWEN_MODEL_API_KEY")
api_base=os.getenv("QWEN_MODEL_BASE_URL")

ua=os.getenv("USER_AGENT")

# åˆå§‹åŒ– LLM æ¨¡åž‹
model = ChatOpenAI(
    model_name= model,
    api_key= api_key,
    base_url=api_base,
)
description = """
è¿™æ˜¯ä¸€ä¸ªå…³äºŽèµ›é©¬å¨˜è§’è‰²ä¿¡æ¯çš„è¡¨æ ¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- èµ›é©¬å¨˜ä¸­æ–‡å: è§’è‰²çš„ä¸­æ–‡åç§°
- èµ›é©¬å¨˜è‹±è¯­å / æ—¥è¯­å: å¯¹åº”è‹±æ–‡å’Œæ—¥æ–‡å
- ç”Ÿæ—¥: å‡ºç”Ÿæ—¥æœŸ
- ä¸‰å›´: BWH å°ºå¯¸
- èº«é«˜(cm): å•ä½åŽ˜ç±³
- å£°ä¼˜(cv): é…éŸ³æ¼”å‘˜
- èµ›é©¬å¨˜ç‰¹æ®Šç§°å·(ä¸­æ–‡å/æ—¥è¯­å): ç‰¹æ®Šç§°å·åŠæ—¥æ–‡åï¼Œä¹Ÿå«ä¸“å±žç§°å·ï¼Œæˆ–äºŒã¤åã€åˆ«ç§°
- èµ›é©¬å¨˜ç‰¹æ®Šç§°å·èŽ·å–æ¡ä»¶: èŽ·å–è¯¥ç§°å·çš„å…·ä½“æ¸¸æˆå†…æ¡ä»¶

ä¾‹å¦‚ï¼šç±³æµ´çš„ç§°å·â€œæ¼†é»‘åˆºå®¢â€çš„èŽ·å–æ¡ä»¶æ˜¯ï¼š
é‡èµæ¯”èµ›å‡ºæˆ˜23æ¬¡ä»¥ä¸Šï¼Œåœ¨äººæ°”ç¬¬äºŒä»¥ä¸Šçš„æƒ…å†µä¸‹å–å¾—èŠèŠ±è³žã€å¤©çš‡è³ž(æ˜¥)çš„èƒœåˆ©ï¼›ç²‰ä¸æ•°è¾¾åˆ°320,000ä»¥ä¸Šã€‚
"""
metadata_doc = Document(page_content=description, metadata={"source": "table_description"})

# åŠ è½½æ–‡æ¡£,å¯æ¢æˆPDFã€txtã€docç­‰å…¶ä»–æ ¼å¼æ–‡æ¡£
loader = CSVLoader('./docs/umamusume_character_baseinfo.csv', encoding='utf-8')
csv_documents = loader.load()
# for doc in csv_documents[:5]: 
#     print("-----")
#     print(doc.page_content)
md_loader = TextLoader('./docs/umamusume_game_info.md', encoding='utf-8')
md_documents = md_loader.load()

documents = [metadata_doc] + csv_documents + md_documents
# text_splitter = RecursiveCharacterTextSplitter.from_language(language="markdown", chunk_size=200, chunk_overlap=0)
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0,separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""])
separators=["\n", "ã€‚", "ï¼Œ", " ", ""]
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    length_function=len,
    add_start_index=True,
    separators=separators,
)


pages = text_splitter.split_documents(documents)

# åŠ è½½PDF
# loader = PyMuPDFLoader("")
# pages = loader.load_and_split()


texts = text_splitter.create_documents(
    [page.page_content for page in pages]
)

# print(f"å…±åˆ‡åˆ†äº† {len(texts)} ä¸ªæ–‡æ¡£å—")
# for i, text in enumerate(texts[:3]):
#     print(f"ç¬¬ {i} ä¸ªæ–‡æœ¬å†…å®¹: {text.page_content[:100]}...")  # æ‰“å°å‰100å­—ç¬¦

# é€‰æ‹©å‘é‡æ¨¡åž‹ï¼Œå¹¶çŒåº“
# oa_embeddings = QwenEmbeddings(
#     model="text-embedding-ada-002",
#     api_key=open_api_key,
#     base_url=openai_api_base,
# )

# qw_embedding_model = QwenEmbeddings(
#     model_name="text-embedding-v2", 
#     api_key=os.getenv("QWEN_API_KEY"),
#     )

hf_embedding_model = HuggingFaceEmbeddings(
    model_name=os.getenv("HF_Embedding_Model"),
    model_kwargs = {'device': 'cpu'},
    encode_kwargs = {'normalize_embeddings': True}
    )
db = FAISS.from_documents(texts, hf_embedding_model)


# èŽ·å–æ£€ç´¢å™¨ï¼Œé€‰æ‹© top-2 ç›¸å…³çš„æ£€ç´¢ç»“æžœ
retriever = db.as_retriever(search_kwargs={"k": 10})

# åˆ›å»ºå¸¦æœ‰ system æ¶ˆæ¯çš„æ¨¡æ¿
prompt_template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªå¯¹æŽ¥é—®é¢˜æŽ’æŸ¥æœºå™¨äººã€‚
               ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›žç­”ç”¨æˆ·é—®é¢˜ã€‚
               ç¡®ä¿ä½ çš„å›žå¤å®Œå…¨ä¾æ®ä¸‹è¿°å·²çŸ¥ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
               è¯·ç”¨ä¸­æ–‡å›žç­”ç”¨æˆ·é—®é¢˜ã€‚

               å·²çŸ¥ä¿¡æ¯:
               {context} """),
    ("user", "{question}")
])

# è‡ªå®šä¹‰çš„æç¤ºè¯å‚æ•°
chain_type_kwargs = {
    "prompt": prompt_template,
}

# å®šä¹‰RetrievalQAé“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=model,
    chain_type="stuff",  # ä½¿ç”¨stuffæ¨¡å¼å°†ä¸Šä¸‹æ–‡æ‹¼æŽ¥åˆ°æç¤ºè¯ä¸­
    chain_type_kwargs=chain_type_kwargs,
    retriever=retriever
)


mcp = FastMCP("RAG Search MCP")


@mcp.tool(description="""
            Use Local Vector Database to answer questions about Umamusume characters.
            contain accurate and basic information about Umamusume characters.
            input: AnswerResponse
            Parameters:{question: str}
            output: AnswerResponse
            Example:{
                "question": "What is the birthday of Rice Shower?"
            }
            Result:{
                "answer": "Rice Shower was born on 3/05."
            }

""")

async def rag(question: str):
    try:
        # èŽ·å–ç”¨æˆ·é—®é¢˜
        user_question = question
        print(user_question)

        # é€šè¿‡RAGé“¾ç”Ÿæˆå›žç­”
        raw_answer = qa_chain.invoke(user_question)

        if isinstance(raw_answer, dict):
            answer_text = raw_answer.get("result", str(raw_answer))  # æå– result å­—æ®µ
        else:
            answer_text = raw_answer

        # è¿”å›žç­”æ¡ˆ
        print(answer_text)
        return {
            "status":"success",
            "result":str(answer_text)
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }        



def create_starlette_app(mcp_server: Server, *, debug: bool = False) -> Starlette:
    sse = SseServerTransport("/messages/")
    session_manager = StreamableHTTPSessionManager(
        app=mcp_server,
        event_store=None,
        json_response=True,
        stateless=True,
    )

    async def handle_sse(request: Request) -> None:
        async with sse.connect_sse(
            request.scope,
            request.receive,
            request._send,
        ) as (read_stream, write_stream):
            await mcp_server.run(
                read_stream,
                write_stream,
                mcp_server.create_initialization_options(),
            )

    async def handle_streamable_http(
        scope: Scope, receive: Receive, send: Send
    ) -> None:
        await session_manager.handle_request(scope, receive, send)

    @contextlib.asynccontextmanager
    async def lifespan(app: Starlette) -> AsyncIterator[None]:
        """Context manager for session manager."""
        async with session_manager.run():
            print("Application started with StreamableHTTP session manager!")
            try:
                yield
            finally:
                print("Application shutting down...")

    return Starlette(
        debug=debug,
        routes=[
            Route("/sse", endpoint=handle_sse),
            Mount("/mcp", app=handle_streamable_http),
            Mount("/messages/", app=sse.handle_post_message),
        ],
        lifespan=lifespan,
    )


# Main entry point
def main():
    mcp_server = mcp._mcp_server

    parser = argparse.ArgumentParser(description="Run Search and Crawl MCP server")

    parser.add_argument(
        "--http",
        action="store_true",
        help="Run the server with Streamable HTTP and SSE transport rather than STDIO (default: False)",
    )
    parser.add_argument(
        "--sse",
        action="store_true",
        help="(Deprecated) An alias for --http (default: False)",
    )
    parser.add_argument(
        "--host", default=None, help="Host to bind to (default: 127.0.0.1)"
    )
    parser.add_argument(
        "--port","-p", type=int, default=None, help="Port to listen on (default: 8887)"
    )
    args = parser.parse_args()
    print(f"ðŸš€ Starting server on port {args.port}")
    use_http = args.http or args.sse

    if not use_http and (args.host or args.port):
        parser.error(
            "Host and port arguments are only valid when using streamable HTTP or SSE transport (see: --http)."
        )
        sys.exit(1)

    if use_http:
        starlette_app = create_starlette_app(mcp_server, debug=True)
        uvicorn.run(
            starlette_app,
            host=args.host if args.host else "127.0.0.1",
            port=args.port if args.port else 7778,
        )
    else:
        mcp.run()


if __name__ == "__main__":
    main()